<template>
  <div class="video-display">
    <!-- ä¸»è§†é¢‘çª—å£ (å¯¹æ–¹ç”»é¢) -->
    <VideoWindow
      ref="remoteWindowRef"
      :stream="remoteStream"
      :autoplay="true"
      :muted="false"
      :mirror="false"
      placeholder-icon="ğŸ“¹"
      placeholder-text="ç­‰å¾…å¯¹æ–¹åŠ å…¥..."
    >
      <!-- é¡¶éƒ¨çŠ¶æ€æ  -->
      <template #overlay>
        <div class="status-bar">
          <span class="status-item">
            <span class="status-dot" :class="{ active: isConnected }"></span>
            {{ isConnected ? 'å·²è¿æ¥' : 'æœªè¿æ¥' }}
          </span>
          <span class="status-item" v-if="duration">
            â±ï¸ {{ formatDuration(duration) }}
          </span>
        </div>
      </template>

      <!-- è‡ªå®šä¹‰å†…å®¹: äººè„¸è¯†åˆ«æ¡†ç­‰ -->
      <template #content>
        <slot name="remote-content"></slot>
      </template>
    </VideoWindow>

    <!-- æœ¬åœ°è§†é¢‘å°çª— (è‡ªå·±çš„ç”»é¢) -->
    <div class="local-video-wrapper">
      <VideoWindow
        ref="localWindowRef"
        :stream="localStream"
        :autoplay="true"
        :muted="true"
        :mirror="true"
        :is-small="true"
        placeholder-icon="ğŸ‘¤"
        placeholder-text=""
      >
        <!-- æ§åˆ¶æŒ‰é’® -->
        <template #controls>
          <div class="controls">
            <button 
              class="control-btn"
              :class="{ active: isCameraOn }"
              @click="toggleCamera"
              title="æ‘„åƒå¤´"
            >
              {{ isCameraOn ? 'ğŸ“¹' : 'ğŸ“¹âŒ' }}
            </button>
            <button 
              class="control-btn"
              :class="{ active: isMicOn }"
              @click="toggleMic"
              title="éº¦å…‹é£"
            >
              {{ isMicOn ? 'ğŸ¤' : 'ğŸ¤âŒ' }}
            </button>
          </div>
        </template>

        <!-- è‡ªå®šä¹‰å†…å®¹: æœ¬åœ°è§†é¢‘çš„é¢å¤–ä¿¡æ¯ -->
        <template #content>
          <slot name="local-content"></slot>
        </template>
      </VideoWindow>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted, onUnmounted } from 'vue'
import VideoWindow from './VideoWindow.vue'

interface Props {
  /** æ˜¯å¦è‡ªåŠ¨å¯åŠ¨æ‘„åƒå¤´ */
  autoStart?: boolean
}

const props = withDefaults(defineProps<Props>(), {
  autoStart: true
})

const emit = defineEmits<{
  streamReady: [stream: MediaStream]
  streamError: [error: Error]
}>()

// è§†é¢‘çª—å£å¼•ç”¨
const localWindowRef = ref<InstanceType<typeof VideoWindow> | null>(null)
const remoteWindowRef = ref<InstanceType<typeof VideoWindow> | null>(null)

// åª’ä½“æµ
const localStream = ref<MediaStream | null>(null)
const remoteStream = ref<MediaStream | null>(null)

// æ§åˆ¶çŠ¶æ€
const isCameraOn = ref(false)
const isMicOn = ref(false)
const isConnected = ref(false)
const duration = ref(0)

// äººè„¸è¯†åˆ«ç›¸å…³
const recognitionGap = ref(1000) // é»˜è®¤ 1 ç§’
const localFaces = ref<any[]>([])

let durationInterval: number | null = null
let recognitionInterval: number | null = null

const API_BASE_URL = 'http://localhost:8000'

/**
 * å¯åŠ¨æœ¬åœ°æ‘„åƒå¤´
 */
const startCamera = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: {
        width: { ideal: 1280 },
        height: { ideal: 720 },
        facingMode: 'user'
      },
      audio: true
    })

    localStream.value = stream
    isCameraOn.value = true
    isMicOn.value = true

    emit('streamReady', stream)
  } catch (error) {
    console.error('å¯åŠ¨æ‘„åƒå¤´å¤±è´¥:', error)
    emit('streamError', error as Error)
  }
}

/**
 * åœæ­¢æœ¬åœ°æ‘„åƒå¤´
 */
const stopCamera = () => {
  if (localStream.value) {
    localStream.value.getTracks().forEach(track => track.stop())
    localStream.value = null
  }

  isCameraOn.value = false
  isMicOn.value = false
}

/**
 * åˆ‡æ¢æ‘„åƒå¤´
 */
const toggleCamera = () => {
  if (!localStream.value) {
    startCamera()
    return
  }

  const videoTrack = localStream.value.getVideoTracks()[0]
  if (videoTrack) {
    videoTrack.enabled = !videoTrack.enabled
    isCameraOn.value = videoTrack.enabled
  }
}

/**
 * åˆ‡æ¢éº¦å…‹é£
 */
const toggleMic = () => {
  if (!localStream.value) return

  const audioTrack = localStream.value.getAudioTracks()[0]
  if (audioTrack) {
    audioTrack.enabled = !audioTrack.enabled
    isMicOn.value = audioTrack.enabled
  }
}

/**
 * è®¾ç½®è¿œç¨‹è§†é¢‘æµ
 */
const setRemoteStream = (stream: MediaStream) => {
  remoteStream.value = stream
  isConnected.value = true
  startDurationTimer()
}

/**
 * å¼€å§‹è®¡æ—¶
 */
const startDurationTimer = () => {
  if (durationInterval) return
  
  duration.value = 0
  durationInterval = window.setInterval(() => {
    duration.value++
  }, 1000)
}

/**
 * åœæ­¢è®¡æ—¶
 */
const stopDurationTimer = () => {
  if (durationInterval) {
    clearInterval(durationInterval)
    durationInterval = null
  }
  duration.value = 0
}

/**
 * æ ¼å¼åŒ–æ—¶é•¿
 */
const formatDuration = (seconds: number): string => {
  const hrs = Math.floor(seconds / 3600)
  const mins = Math.floor((seconds % 3600) / 60)
  const secs = seconds % 60

  if (hrs > 0) {
    return `${hrs.toString().padStart(2, '0')}:${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`
  }
  return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`
}

/**
 * è·å–è¯†åˆ«é—´éš”æ—¶é—´
 */
const fetchRecognitionGap = async () => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/cv/recognition-gap`)
    const data = await response.json()
    recognitionGap.value = data.recognition_gap || 1000
    console.log('è¯†åˆ«é—´éš”:', recognitionGap.value, 'ms')
  } catch (error) {
    console.error('è·å–è¯†åˆ«é—´éš”å¤±è´¥:', error)
    recognitionGap.value = 1000 // ä½¿ç”¨é»˜è®¤å€¼
  }
}

/**
 * è°ƒç”¨äººè„¸è¯†åˆ« API
 */
const recognizeFaces = async () => {
  if (!localWindowRef.value) return
  
  try {
    // ä»è§†é¢‘æ•è·å½“å‰å¸§
    const blob = await localWindowRef.value.captureFrameAsync()
    if (!blob) return
    
    // åˆ›å»º FormData
    const formData = new FormData()
    formData.append('file', blob, 'frame.jpg')
    
    // è°ƒç”¨è¯†åˆ« API
    const response = await fetch(`${API_BASE_URL}/api/cv/recognize-faces`, {
      method: 'POST',
      body: formData
    })
    
    if (!response.ok) {
      console.error('è¯†åˆ«å¤±è´¥:', response.statusText)
      return
    }
    
    const data = await response.json()
    console.log('ğŸ“¸ äººè„¸è¯†åˆ«ç»“æœ:', data)
    
    // æ›´æ–°äººè„¸æ•°æ®
    if (data.faces && data.faces.length > 0) {
      localFaces.value = data.faces
      console.log(`âœ“ æ£€æµ‹åˆ° ${data.faces.length} ä¸ªäººè„¸ï¼Œå¼€å§‹ç»˜åˆ¶...`)
      
      // åœ¨ Canvas ä¸Šç»˜åˆ¶äººè„¸æ¡†
      localWindowRef.value.drawFaces(data.faces)
    } else {
      // æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸,æ¸…ç©º Canvas
      console.log('â„¹ï¸  æœªæ£€æµ‹åˆ°äººè„¸')
      localFaces.value = []
      localWindowRef.value.clearCanvas()
    }
  } catch (error) {
    console.error('äººè„¸è¯†åˆ«å‡ºé”™:', error)
  }
}

/**
 * å¯åŠ¨äººè„¸è¯†åˆ«å®šæ—¶å™¨
 */
const startFaceRecognition = () => {
  if (recognitionInterval) return
  
  console.log('å¯åŠ¨äººè„¸è¯†åˆ«,é—´éš”:', recognitionGap.value, 'ms')
  
  // ç«‹å³æ‰§è¡Œä¸€æ¬¡
  recognizeFaces()
  
  // è®¾ç½®å®šæ—¶å™¨
  recognitionInterval = window.setInterval(() => {
    recognizeFaces()
  }, recognitionGap.value)
}

/**
 * åœæ­¢äººè„¸è¯†åˆ«å®šæ—¶å™¨
 */
const stopFaceRecognition = () => {
  if (recognitionInterval) {
    clearInterval(recognitionInterval)
    recognitionInterval = null
    console.log('åœæ­¢äººè„¸è¯†åˆ«')
  }
  
  // æ¸…ç©ºäººè„¸æ•°æ®å’Œ Canvas
  localFaces.value = []
  if (localWindowRef.value) {
    localWindowRef.value.clearCanvas()
  }
}

onMounted(async () => {
  // è·å–è¯†åˆ«é—´éš”
  await fetchRecognitionGap()
  
  // å¯åŠ¨æ‘„åƒå¤´
  if (props.autoStart) {
    await startCamera()
    
    // ç­‰å¾…è§†é¢‘æµå‡†å¤‡å¥½åå¯åŠ¨äººè„¸è¯†åˆ«
    setTimeout(() => {
      startFaceRecognition()
    }, 1000)
  }
})

onUnmounted(() => {
  stopCamera()
  stopDurationTimer()
  stopFaceRecognition()
})

// æš´éœ²æ–¹æ³•ç»™çˆ¶ç»„ä»¶
defineExpose({
  startCamera,
  stopCamera,
  toggleCamera,
  toggleMic,
  setRemoteStream,
  getLocalStream: () => localStream.value,
  getRemoteStream: () => remoteStream.value,
  startFaceRecognition,
  stopFaceRecognition,
  recognizeFaces
})
</script>

<style scoped>
.video-display {
  position: relative;
  width: 100%;
  height: 100%;
  background: #1a1a1a;
  border-radius: 8px;
  overflow: hidden;
}

/* æœ¬åœ°è§†é¢‘å°çª—ä½ç½® */
.local-video-wrapper {
  position: absolute;
  bottom: 20px;
  right: 20px;
  width: 240px;
  height: 180px;
  z-index: 100;
}

/* çŠ¶æ€æ æ ·å¼ */
.status-bar {
  margin: 16px;
  display: inline-flex;
  gap: 16px;
  padding: 8px 16px;
  background: rgba(0, 0, 0, 0.6);
  border-radius: 20px;
  backdrop-filter: blur(10px);
  font-size: 14px;
  color: #fff;
}

.status-item {
  display: flex;
  align-items: center;
  gap: 6px;
}

.status-dot {
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background: #666;
  transition: background 0.3s ease;
}

.status-dot.active {
  background: #22c55e;
  box-shadow: 0 0 8px #22c55e;
}

/* æ§åˆ¶æŒ‰é’®æ ·å¼ */
.controls {
  display: flex;
  gap: 8px;
  justify-content: center;
  padding: 8px;
  opacity: 0;
  transition: opacity 0.3s ease;
}

.local-video-wrapper:hover .controls {
  opacity: 1;
}

.control-btn {
  width: 36px;
  height: 36px;
  border-radius: 50%;
  border: none;
  background: rgba(0, 0, 0, 0.7);
  color: white;
  font-size: 16px;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.2s ease;
  backdrop-filter: blur(10px);
}

.control-btn:hover {
  background: rgba(0, 0, 0, 0.9);
  transform: scale(1.1);
}

.control-btn.active {
  background: rgba(59, 130, 246, 0.8);
}

/* å“åº”å¼è°ƒæ•´ */
@media (max-width: 768px) {
  .local-video-wrapper {
    width: 160px;
    height: 120px;
    bottom: 12px;
    right: 12px;
  }

  .status-bar {
    font-size: 12px;
    padding: 6px 12px;
    margin: 12px;
  }
}
</style>

